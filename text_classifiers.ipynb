{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Text Classifiers\n",
    "\n",
    "Frank Neugebauer\n",
    "October 1, 2019\n",
    "\n",
    "The objective of this project is to demonstrate the accuracy of different text classifiers in Python. To get that output, corpora from Reddit that show categorized and controversial entries is used.\n",
    "\n",
    "Some of what's demonstrated:\n",
    "* Reading JSON files\n",
    "* Sampling to increase performance\n",
    "* Tokenization\n",
    "* Creating vectors as features\n",
    "* Logistic regression with different penalities\n",
    "* Multinomrial Naive Bayes\n",
    "\n",
    "First, import everything that's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Sampling Data\n",
    "\n",
    "The next step is to load the data, but it's massive and in order to avoid processing problems, I load the data and take a sample of 1000 from each. Then, that sample data is saved as separate CSV files.\n",
    "\n",
    "Only the small samples have been uploaded, so do not run this (unless, of course, you have the full JSON files noted in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO NEED TO RUN THIS, JUST LOAD THE CSV(s) IN THE NEXT BLOCK!\n"
     ]
    }
   ],
   "source": [
    "#cat_comments = pd.read_json('data/categorized-comments.jsonl', lines=True)\n",
    "#cont_comments = pd.read_json('data/controversial-comments.jsonl', lines=True)\n",
    "\n",
    "#small_cat_comments = cat_comments.sample(n=1000)\n",
    "#small_cont_comments = cont_comments.sample(n=1000)\n",
    "\n",
    "#small_cat_comments.to_csv(r'data/small_cat_comments.csv')\n",
    "#small_cont_comments.to_csv(r'data/small_cont_comments.csv')\n",
    "print(\"NO NEED TO RUN THIS, JUST LOAD THE CSV(s) IN THE NEXT BLOCK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid loading the entire data set each time, this code block independently loads the sample CSV files. This means that the previous step can be skipped every time except the first time (or any time you change the sample size).\n",
    "\n",
    "This code should always work because at a minimum, the initial 1,000 observation files should be there (e.g., `small_cat_comments.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_comments = pd.read_csv(r'data/small_cat_comments.csv')\n",
    "#cont_comments = pd.read_csv(r'data/small_cont_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Target and Features\n",
    "\n",
    "Within the categorical data set, the target is the `cat` (category) column and the feature (there's only one) is the `txt` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sample:\n",
      " 0               video_games\n",
      "1    science_and_technology\n",
      "2                    sports\n",
      "3                    sports\n",
      "4               video_games\n",
      "Name: cat, dtype: object \n",
      "\n",
      "Feature sample:\n",
      " 0    I gotta say, Nintendo knocked it out of the pa...\n",
      "1    What precious thing did you want to post that ...\n",
      "2               Freeney sack@!\\n\\nGood job old timer!!\n",
      "3    Don't blow it....keep it simple.... count your...\n",
      "4    Which platform? When? Can't play till Friday p...\n",
      "Name: txt, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cat_target = cat_comments['cat']\n",
    "cat_features = cat_comments['txt']\n",
    "\n",
    "# Show a little of the features and target\n",
    "print(\"Target sample:\\n\", cat_target[0:5], \"\\n\")\n",
    "print(\"Feature sample:\\n\", cat_features[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to Train and Test Sets\n",
    "\n",
    "The train set should be bigger since it's used to build the model, whereas the test set is used to validate the model with new data. For that reason, the test data is only 25% of the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = \\\n",
    "        train_test_split(cat_features, cat_target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model and Predictions\n",
    "\n",
    "A pipeline is used and passed a TfidfVectorizer and MultinomailNB (Naive Bayes) objects. The pipeline, in turn, makes the words a vector of numbers so Naive Bayes can be used to predict the category.\n",
    "\n",
    "In English, the sentences are broken down into words, which are the converted to numeric equivalents. Then, new sentences are broken down to numbers and compared to the numbers already created. When the algorithm finds the closest match, it chooses the category from that match as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small sample of the categorical predictions: ['video_games' 'video_games' 'video_games' 'video_games' 'sports']\n"
     ]
    }
   ],
   "source": [
    "nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "nb_model.fit(features_train, target_train)\n",
    "nb_test_predictions = nb_model.predict(features_test)\n",
    "\n",
    "print(\"Small sample of the categorical predictions:\", nb_test_predictions[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Understanding how accurate the test is matters greatly and indicates how well a model will perform in the \"real world.\" The lower the accuracy, the less likely the model will accurately predict a new sample. Unfortunately, the Naive Bayes model did not predict well, although that's likely for several reasons, including:\n",
    "\n",
    "* The sample size is very small\n",
    "* The text may require more tweaking (e.g., removing stop words and punctuation)\n",
    "* Yeah, I think that's all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the categorical test is: 0.496\n"
     ]
    }
   ],
   "source": [
    "cat_accuracy_test = accuracy_score(target_test, nb_test_predictions)\n",
    "print(\"The accuracy for the categorical test is:\", cat_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Sample Predictions\n",
    "\n",
    "Putting this in context, this model can be used as the 'engine' to make predictions based on new data. Taking a step back, in theory, the corpus and prediction can be anything you have the right data for - in this case, the data was great because every category comment had a category and every controversial commenet was noted as such. Without that level of detail, this engine would not be possible because you could not train a model as shown.\n",
    "\n",
    "Here I'll take the same comment and run it through both the category and controvery models to see if works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The easy predicted category is: ['sports']\n",
      "The hard predicted category is: ['video_games']\n"
     ]
    }
   ],
   "source": [
    "new_data_easy = \"I love football!\"\n",
    "new_data_hard = \"I really love this game. Football is amazing!\"\n",
    "\n",
    "pred = nb_model.predict([new_data_easy])\n",
    "print(\"The easy predicted category is:\", pred)\n",
    "\n",
    "pred = nb_model.predict([new_data_hard])\n",
    "print(\"The hard predicted category is:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The challenge (in particular) with the \"hard\" new data is that both `game` and `football` were within the text. This problem can be solved with more sample, although this illustrates the fundamental nature of these kinds of models; they require data. \n",
    "\n",
    "If this were a production application, the new sample (i.e., txt=\"I really love this game. Football is amazing!\" cat=\"sports\") would be added to the corpus so next time the model was generated, it would work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
