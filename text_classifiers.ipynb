{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Text Classifiers\n",
    "\n",
    "Frank Neugebauer\n",
    "October 1, 2019\n",
    "\n",
    "The objective of this project is to demonstrate the accuracy of different text classifiers in Python. To get that output, corpora from Reddit that show categorized and controversial entries is used.\n",
    "\n",
    "Some of what's demonstrated:\n",
    "* Reading JSON files\n",
    "* Sampling to increase performance\n",
    "* Tokenization\n",
    "* Creating vectors as features\n",
    "* Logistic regression with different penalities\n",
    "* Multinomrial Naive Bayes\n",
    "\n",
    "First, import everything that's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Sampling Data\n",
    "\n",
    "The next step is to load the data, but it's massive and in order to avoid processing problems, I loaded the full data outside of GitHub and then took a sample from each. Then, that sample data is saved as separate CSV files.\n",
    "\n",
    "Note that the controversial data set `sample` method was not balancing the sample by default, which is why I setup criteria for it. \n",
    "\n",
    "**The samples are already setup in the .csv files. You don't need to run this block of code (it won't even work) unless you upload the full .jsonl files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO NEED TO RUN THIS, JUST LOAD THE CSV(s) IN THE NEXT BLOCK!\n"
     ]
    }
   ],
   "source": [
    "cat_comments = pd.read_json('data/categorized-comments.jsonl', lines=True)\n",
    "cont_comments = pd.read_json('data/controversial-comments.jsonl', lines=True)\n",
    "\n",
    "df1 = cont_comments[cont_comments['con']==1].sample(n=2500)\n",
    "df0 = cont_comments[cont_comments['con']==0].sample(n=2500)\n",
    "small_cont_comments = pd.concat([df1,df0])\n",
    "\n",
    "small_cont_comments = cont_comments[cont_comments].sample(n=5000)\n",
    "small_cat_comments = cat_comments.sample(n=5000)\n",
    "\n",
    "small_cont_comments.to_csv(r'data/small_cont_comments.csv')\n",
    "small_cat_comments.to_csv(r'data/small_cat_comments.csv')\n",
    "\n",
    "print(\"NO NEED TO RUN THIS, JUST LOAD THE CSV(s) IN THE NEXT BLOCK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid loading the entire data set each time, this code block independently loads the sample CSV files. This means that the previous step can be skipped every time except the first time (or any time you change the sample size).\n",
    "\n",
    "This code should always work because at a minimum, the initial 1,000 observation files should be there (e.g., `small_cat_comments.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_comments = pd.read_csv(r'data/small_cat_comments.csv')\n",
    "cont_comments = pd.read_csv(r'data/small_cont_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Target and Features\n",
    "\n",
    "Within the categorical data set, the target is the `cat` (category) column and the feature (there's only one) is the `txt` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical target sample:\n",
      " 0               video_games\n",
      "1    science_and_technology\n",
      "2                    sports\n",
      "3                    sports\n",
      "4               video_games\n",
      "Name: cat, dtype: object \n",
      "\n",
      "Categorical feature sample:\n",
      " 0    I gotta say, Nintendo knocked it out of the pa...\n",
      "1    What precious thing did you want to post that ...\n",
      "2               Freeney sack@!\\n\\nGood job old timer!!\n",
      "3    Don't blow it....keep it simple.... count your...\n",
      "4    Which platform? When? Can't play till Friday p...\n",
      "Name: txt, dtype: object \n",
      "\n",
      "Controversial target sample:\n",
      " 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: con, dtype: int64 \n",
      "\n",
      "Controversial feature sample:\n",
      " 0    Why is the a \"bombshell?\" If true and not anot...\n",
      "1                               /s\\n\\nAt least I hope.\n",
      "2    Well, theoretically the chances of democrats w...\n",
      "3                         They always do unfortunately\n",
      "4    After 2 years on it with crazy high deductible...\n",
      "Name: txt, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cat_target = cat_comments['cat']\n",
    "cat_features = cat_comments['txt']\n",
    "cont_target = cont_comments['con']\n",
    "cont_features = cont_comments['txt']\n",
    "\n",
    "# Show a little of the features and target\n",
    "print(\"Categorical target sample:\\n\", cat_target[0:5], \"\\n\")\n",
    "print(\"Categorical feature sample:\\n\", cat_features[0:5], \"\\n\")\n",
    "print(\"Controversial target sample:\\n\", cont_target[0:5], \"\\n\")\n",
    "print(\"Controversial feature sample:\\n\", cont_features[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to Train and Test Sets\n",
    "\n",
    "The train set should be bigger since it's used to build the model, whereas the test set is used to validate the model with new data. For that reason, the test data is only 25% of the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_train, cat_features_test, cat_target_train, cat_target_test = \\\n",
    "        train_test_split(cat_features, cat_target, test_size=0.25)\n",
    "\n",
    "cont_features_train, cont_features_test, cont_target_train, cont_target_test = \\\n",
    "        train_test_split(cont_features, cont_target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model and Predictions\n",
    "\n",
    "A pipeline is used and passed a TfidfVectorizer and MultinomailNB (Naive Bayes) objects. The pipeline, in turn, makes the words a vector of numbers so Naive Bayes can be used to predict the category.\n",
    "\n",
    "In English, the sentences are broken down into words, which are the converted to numeric equivalents. Then, new sentences are broken down to numbers and compared to the numbers already created. When the algorithm finds the closest match, it chooses the category from that match as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small sample of the categorical predictions: ['video_games' 'video_games' 'video_games' 'sports' 'sports']\n",
      "Small sample of the controversial predictions: [1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "nb_model.fit(cat_features_train, cat_target_train)\n",
    "nb_test_predictions = nb_model.predict(cat_features_test)\n",
    "\n",
    "print(\"Small sample of the categorical predictions:\", nb_test_predictions[0:5])\n",
    "\n",
    "logistic_model = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "logistic_model.fit(cont_features_train, cont_target_train)\n",
    "logistic_test_predictions = logistic_model.predict(cont_features_test)\n",
    "\n",
    "print(\"Small sample of the controversial predictions:\", logistic_test_predictions[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Understanding how accurate the test is matters greatly and indicates how well a model will perform in the \"real world.\" The lower the accuracy, the less likely the model will accurately predict a new sample. Unfortunately, the Naive Bayes model did not predict well, although that's likely for several reasons, including:\n",
    "\n",
    "* The sample size is very small\n",
    "* The text may require more tweaking (e.g., removing stop words and punctuation)\n",
    "* Yeah, I think that's all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the categorical test is: 0.492\n",
      "The accuracy for the controversial test is: 0.5576\n"
     ]
    }
   ],
   "source": [
    "cat_accuracy_test = accuracy_score(cat_target_test, nb_test_predictions)\n",
    "print(\"The accuracy for the categorical test is:\", cat_accuracy_test)\n",
    "\n",
    "cont_accuracy_test = accuracy_score(cont_target_test, logistic_test_predictions)\n",
    "print(\"The accuracy for the controversial test is:\", cont_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Sample Predictions\n",
    "\n",
    "Putting this in context, this model can be used as the 'engine' to make predictions based on new data. Taking a step back, in theory, the corpus and prediction can be anything you have the right data for - in this case, the data was great because every category comment had a category and every controversial commenet was noted as such. Without that level of detail, this engine would not be possible because you could not train a model as shown.\n",
    "\n",
    "Here I'll take the same comment and run it through both the category and controvery models to see if works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The easy predicted category is: ['sports']\n",
      "The hard predicted category is: ['video_games']\n",
      "The non-controversial comment prediction: [0]\n",
      "The controversial comment prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "new_data_easy = \"I love football!\"\n",
    "new_data_hard = \"I'm a gamer for life!\"\n",
    "\n",
    "cat_pred = nb_model.predict([new_data_easy])\n",
    "print(\"The easy predicted category is:\", cat_pred)\n",
    "\n",
    "cat_pred = nb_model.predict([new_data_hard])\n",
    "print(\"The hard predicted category is:\", cat_pred)\n",
    "\n",
    "# The controversial test\n",
    "new_data_nc = \"Some vanilla statement, not controversial.\"\n",
    "new_data_cont = \"Abortion should be legal.\"\n",
    "\n",
    "cont_pred = logistic_model.predict([new_data_nc])\n",
    "print(\"The non-controversial comment prediction:\", cont_pred)\n",
    "\n",
    "cont_pred = logistic_model.predict([new_data_cont])\n",
    "print(\"The controversial comment prediction:\", cont_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "If this were a production application, any new sample that the system incorrectly predicted (e.g., txt=\"I really love this game. Football is amazing!\" cat=\"sports\") would be added to the corpus so next time the model was generated, it would work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
