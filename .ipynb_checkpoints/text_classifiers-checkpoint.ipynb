{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Text Classifiers\n",
    "\n",
    "Frank Neugebauer\n",
    "March 24, 2019\n",
    "\n",
    "The objective of this project is to demonstrate the accuracy of different text classifiers in Python. To get that output, corpora from Reddit that show categorized and controversial entries is used.\n",
    "\n",
    "Some of what's demonstrated:\n",
    "* Reading JSON files\n",
    "* Sampling to increase performance\n",
    "* Tokenization\n",
    "* Creating vectors as features\n",
    "* Logistic regression with different penalities\n",
    "* Multinomrial Naive Bayes\n",
    "\n",
    "First, import everything that's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN THIS BLOCK\n",
    "The next step is to load the data, but it's massive and in order to avoid processing problems, I load the data and take a sample of 1000 from each. Then, that sample data is saved as separate CSV files.\n",
    "\n",
    "Only the small samples have been uploaded, so do not run this (unless, of course, you have the full JSON files noted in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_comments = pd.read_json('data/categorized-comments.jsonl', lines=True)\n",
    "cont_comments = pd.read_json('data/controversial-comments.jsonl', lines=True)\n",
    "\n",
    "small_cat_comments = cat_comments.sample(n=1000)\n",
    "small_cont_comments = cont_comments.sample(n=1000)\n",
    "\n",
    "small_cat_comments.to_csv(r'data/small_cat_comments.csv')\n",
    "small_cont_comments.to_csv(r'data/small_cont_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid loading the entire data set each time, this code block independently loads the sample CSV files. This means that the previous step can be skipped every time except the first time (or any time you change the sample size).\n",
    "\n",
    "This code should always work because at a minimum, the initial 1,000 observation files should be there (e.g., `small_cat_comments.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_comments = pd.read_csv(r'data/small_cat_comments.csv')\n",
    "cont_comments = pd.read_csv(r'data/small_cont_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpora - one for the categorized text, the other for controversial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What precious thing did you want to post that you can't?\\n\\n\", 'Freeney sack@!\\n\\nGood job old timer!!', \"Don't blow it....keep it simple.... count your money \", \"Which platform? When? Can't play till Friday probably. Even if I have the freaking season pass :(\"]\n"
     ]
    }
   ],
   "source": [
    "cat_comments_only = cat_comments['txt']\n",
    "cont_comments_only = cont_comments['txt']\n",
    "cat_corpus = cat_comments_only.tolist()\n",
    "cont_corpus = cont_comments_only.tolist()\n",
    "\n",
    "# Show a little of one of the corpora unfiltered\n",
    "print(cat_corpus[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the corpus is still intact; it includes stop words and punctuation - and even newline sequences. This can be wasteful depending on your objectives. In this case, stop words (e.g., 'the') can be removed since they don't indicate caegories or controversy (intuitively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I got ta say , Nintendo knocked park getting games like lined fill gap Zelda Mario What precious thing want post ca n't ? \", \"I got ta say , Nintendo knocked park getting games like lined fill gap Zelda Mario What precious thing want post ca n't ? Freeney sack @ ! Good job old timer ! ! \", \"I got ta say , Nintendo knocked park getting games like lined fill gap Zelda Mario What precious thing want post ca n't ? Freeney sack @ ! Good job old timer ! ! Do n't blow ... .keep simple ... . count money \", \"I got ta say , Nintendo knocked park getting games like lined fill gap Zelda Mario What precious thing want post ca n't ? Freeney sack @ ! Good job old timer ! ! Do n't blow ... .keep simple ... . count money Which platform ? When ? Ca n't play till Friday probably . Even I freaking season pass : ( \"]\n"
     ]
    }
   ],
   "source": [
    "def clean_corpus(corpus):\n",
    "    word_tokens = []\n",
    "    for sentence in corpus:\n",
    "        word_tokens.append(word_tokenize(sentence))\n",
    "\n",
    "    filtered_sentences = []\n",
    "    for tokenized_sentence in word_tokens:\n",
    "        filtered_sentence = []\n",
    "        for word in tokenized_sentence:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    concat_sent = ''\n",
    "    final_corpus = []\n",
    "    for filtered_word in filtered_sentences:\n",
    "        for element in filtered_word:\n",
    "            concat_sent += str(element)\n",
    "            concat_sent += ' '\n",
    "        final_corpus.append(concat_sent)\n",
    "    \n",
    "    return final_corpus\n",
    "    \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "cat_corpus_clean = clean_corpus(cat_corpus)\n",
    "cont_corpus_clean = clean_corpus(cont_corpus)\n",
    "\n",
    "# Show a little of the filtered corpus\n",
    "print(cat_corpus_clean[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vectorizer = TfidfVectorizer()\n",
    "cont_vectorizer = TfidfVectorizer()\n",
    "cat_vector = cat_vectorizer.fit_transform(cat_corpus_clean)\n",
    "cont_vector = cont_vectorizer.fit_transform(cont_corpus_clean)\n",
    "\n",
    "cat_features = cat_vector.toarray()\n",
    "cont_features = cont_vector.toarray()\n",
    "cat_target = cat_comments['cat']\n",
    "cont_target = cont_comments['con']\n",
    "\n",
    "accuracy_df = pd.DataFrame(columns=['Model', 'Data Set', 'Accuracy(Train)', 'Accuracy(Test)'])\n",
    "\n",
    "def createClassifier(model_txt, data_txt, target, features, test_size, classifier):\n",
    "    features_train, features_test, target_train, target_test = \\\n",
    "        train_test_split(features, target, test_size=test_size)\n",
    "\n",
    "    model = classifier.fit(features_train, target_train)\n",
    "    test_predictions = model.predict(features_test)\n",
    "    train_predictions = model.predict(features_train)\n",
    "\n",
    "    accuracy_test = accuracy_score(target_test, test_predictions)\n",
    "    accuracy_train = accuracy_score(target_train, train_predictions)\n",
    "\n",
    "    new_df = accuracy_df.append({'Model': model_txt, 'Data Set':data_txt, 'Accuracy(Train)':accuracy_train,\n",
    "                        'Accuracy(Test)':accuracy_test}, ignore_index=True)\n",
    "    print(new_df)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the reusable function created, call it for each variation, for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model     Data Set  Accuracy(Train)  Accuracy(Test)\n",
      "0  LR (L1)  Controversy         0.958667           0.956\n",
      "     Model     Data Set  Accuracy(Train)  Accuracy(Test)\n",
      "0  LR (L2)  Controversy         0.957333            0.96\n",
      "     Model     Data Set  Accuracy(Train)  Accuracy(Test)\n",
      "0  NB       Controversy            0.956           0.964\n",
      "     Model    Data Set  Accuracy(Train)  Accuracy(Test)\n",
      "0  LR (L1)  Categories         0.426667           0.468\n",
      "      Model    Data Set  Accuracy(Train)  Accuracy(Test)\n",
      "0  LRn (L2)  Categories             0.46           0.392\n",
      "      Model    Data Set  Accuracy(Train)  Accuracy(Test)\n",
      "0  NB        Categories            0.408            0.48\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0, penalty='l1', solver='liblinear')\n",
    "model_cont_lr_l1 = createClassifier('LR (L1)', 'Controversy', cont_target, cont_features, 0.25, classifier)\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, penalty='l2', solver='liblinear')\n",
    "model_cont_lr_l2 = createClassifier('LR (L2)', 'Controversy', cont_target, cont_features, 0.25, classifier)\n",
    "\n",
    "classifier = MultinomialNB(class_prior=[0.25, 0.5])\n",
    "model_cont_nb = createClassifier('NB     ', 'Controversy', cont_target, cont_features, 0.25, classifier)\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, penalty='l1', solver='liblinear', multi_class='ovr')\n",
    "model_cat_lr_l1 = createClassifier('LR (L1)', 'Categories', cat_target, cat_features, 0.25, classifier)\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, penalty='l2', solver='liblinear', multi_class='ovr')\n",
    "model_cat_lr_l2 = createClassifier('LRn (L2)', 'Categories', cat_target, cat_features, 0.25, classifier)\n",
    "\n",
    "classifier = MultinomialNB(class_prior=[0.25, 0.25, .25, .25])\n",
    "model_cat_nb = createClassifier('NB      ', 'Categories', cat_target, cat_features, 0.25, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Sample Predictions\n",
    "\n",
    "Putting this in context, this model can be used as the 'engine' to make predictions based on new data. Taking a step back, in theory, the corpus and prediction can be anything you have the right data for - in this case, the data was great because every category comment had a category and every controversial commenet was noted as such. Without that level of detail, this engine would not be possible because you could not train a model as shown.\n",
    "\n",
    "Here I'll take the same comment and run it through both the category and controvery models to see if works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the Wondaland football team really sucks\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features per sample; expecting 5327",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-5241d9e8f449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfoo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthe_new_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel_cont_lr_l1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py35\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 305\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1 features per sample; expecting 5327"
     ]
    }
   ],
   "source": [
    "#the_new_comment = np.array(word_tokenize(\"I think the Wonkaland football team really sucks.\"))\n",
    "the_new_comment = \"I think the Wondaland football team really sucks\"\n",
    "\n",
    "print(the_new_comment)\n",
    "foo = np.array(the_new_comment)\n",
    "\n",
    "model_cont_lr_l1.predict(foo.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
